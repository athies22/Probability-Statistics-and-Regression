\documentclass{amsart}
	%% Basic Info
		\author{Alex Thies}
		\title{Final Exam \\ Math 463 - Spring 2017}
		\email{athies@uoregon.edu}
	%% boilerplate packages
		\usepackage{amsmath}
		\usepackage{amssymb}
		\usepackage{amsthm}
		\usepackage{enumerate}
		\usepackage{mathrsfs}
		\usepackage{color}
		\usepackage{hyperref}
	%% rename the abstract
		\renewcommand{\abstractname}{Introduction}
	%% my shorthand
		% sets
		\DeclareMathOperator{\Z}{\mathbb{Z}}
		\DeclareMathOperator{\Zp}{\mathbb{Z^{+}}}
		\DeclareMathOperator{\Zm}{\mathbb{Z^{-}}}
		\DeclareMathOperator{\N}{\mathbb{Z^{+}}}
		% linear algebra stuff
		\DeclareMathOperator{\Ell}{\mathscr{L}}
		% stats stuff
		\DeclareMathOperator{\var}{\rm Var}
		\DeclareMathOperator{\cov}{\rm Cov}
		\DeclareMathOperator{\sd}{\rm SD}
		\DeclareMathOperator{\SE}{\rm SE}
		\DeclareMathOperator{\ind}{\perp\!\!\!\perp}
		\DeclareMathOperator{\Prob}{\mathbb{P}}
		% use pretty characters
		\DeclareMathOperator{\ep}{\varepsilon}
		\DeclareMathOperator{\ph}{\varphi}
	%% Levin's shorthand
		\newcommand{\E}{{\mathcal{E}}}
		\newcommand{\A}{{\mathcal{A}}}
		\newcommand{\B}{{\mathcal{B}}}
		\newcommand{\R}{{\mathbb{R}}}
		\newcommand{\X}{{\mathbf{X}}}
		\newcommand{\x}{{\mathbf{x}}}
		\newcommand{\M}{{\mathcal{M}}}
		\newcommand{\bvec}[1]{{\boldsymbol #1}}
		\newcommand{\bbeta}{\bvec{\beta}}
		\newcommand{\bX}{\bvec{X}}
		\newcommand{\bY}{\bvec{Y}}
		\newcommand{\ssreg}{{\rm SS}_{{\rm Reg}}}
		\newcommand{\ssr}{{\rm SS}_{{\rm Res}}}
		\newcommand{\sst}{{\rm SS}_{{\rm Tot}}}
\begin{document}
	\begin{abstract}
		By submitting this final, I certify that I have followed exactly the rules outlined on the front of the exam.
	\end{abstract}

	\maketitle
<<0.Basic stuff, include=FALSE>>=
library(knitr)
library(xtable)
options(digits=2)
@
	\section{Assignment} % (fold)
	\label{sec:assignment}
		\subsection{Problem 1} % (fold)
		\label{sub:problem_1}
		Consider the data available at
<<1.Data>>=
rm(list=ls())
ozone <- read.table("http://pages.uoregon.edu/dlevin/DATA/ozo.txt",
                    header=T)
@
		The variables \verb|temp| and \verb|humidity| give daily temperature and humidity readings.
		The variable \verb|HO| indicates if ozone levels are high.
		Use the data to fit a probit model:
		Here let $Y_i = 1$ if and only if the ozone level is high, and write $\bvec{x}^{(i)} = (1, {\tt temp}_i, {\tt humidity}_i)$.
		\[
		\Prob(Y_i = 1 \mid \bvec{x}^{(i)}) = \Phi( \bvec{x}^{(i)} \bvec{\beta}) \,,
		\]
		where $\Phi$ is the normal cdf.
		Provide the fitted coefficients and their standard errors.
		(The R function \verb|glm| can be used to fit a probit. Be sure to specify \verb|family=binomial(link="probit")|.)
		\begin{enumerate}[(a)]
		\item What is the estimated probability of a high ozone day if the temperature is 95 degrees and the humidity is 80\%?
		\item Find a 95\% confidence interval for the linear predictor $\beta_0 + 95\beta_1 + 80\beta_2$ at these values.
		\item Find a 95\% confidence interval for the probability  of high ozone at these values.
		\end{enumerate}
		Note that if \verb|f| is the fitted probit model (using \verb|glm|), then \verb|summary(f)$cov.unscaled| gives the approximate covariance matrix $\cov(\hat{\bvec{\beta}})$.
		Alternatively, \verb|predict| can give fitted values and their standard errors, for given covariates.
		\begin{proof}[Solution] \
<<1.Work, include=FALSE>>=
ozone$HO <- as.numeric(ozone$HO)
mod1 <- glm(HO~temp+humidity, data = ozone, family = binomial(link = "probit"))
summary(mod1)
cov <- summary(mod1)$cov.unscaled
x0 <- mod1$coefficients[1]
x1 <- mod1$coefficients[2]
x2 <- mod1$coefficients[3]
hum <- 80
temp <- 95

#### 1a ####
XB <- x0 + x1*temp + x2*hum
y <- pnorm(XB, mean = 0, sd = 1)

#### 1b ####
a <- cbind(1,95,80)
at <- t(a)
sig <- (a %*% cov %*% at)**(0.5)
CI1low <- XB - 1.96*sig[1]
CI1high <- XB + 1.96*sig[1]

#### 1c ####
CI2low <- pnorm(CI1low)
CI2high <- pnorm(CI1high)
@
			\begin{enumerate}[(a)]
				\item We compute the following in R,
				    \begin{align*}
				        \Prob(X\beta | \beta_{\rm Temp} = 95, \beta_{\rm Humid} = 80) &= \Phi(x_{0} + \beta_{\rm Temp}x_{1} + \beta_{\rm Humid}x_{2}), \\
				        &= \Sexpr{y}.
				    \end{align*}
				\item We compute a 95\% following confidence interval for $X\beta$ given the specified values for temperature and humidity in R, $$\left( \Sexpr{CI1low}, \Sexpr{CI1high} \right).$$
				\item Utilizing the endpoints of the previous confidence interval, we compute the following 95\% confidence interval for high ozone given the values of temperature and humidity, $$ \left( \Phi(\Sexpr{CI1low}), \Phi(\Sexpr{CI1high}) \right) = \left( \Sexpr{CI2low}, \Sexpr{CI2high} \right).$$
			\end{enumerate}
		\end{proof}
		% subsubsection problem_1 (end)
		\newpage

		\subsection{Problem 2} % (fold)
		\label{sub:problem_2}
<<2.Data, echo = TRUE, fig = TRUE, include = FALSE>>=
rm(list=ls())
di = read.table("http://pages.uoregon.edu/dlevin/DATA/DI.txt",
                header=T,row.names=1,sep="\t")
mod <- lm(NDIR~.-NDIR, data = di)
@
        \begin{figure}[h]
<<2.Fig1, out.width='.7\\linewidth', echo = FALSE, fig = TRUE>>=
plot(NDIR~Taxes, data=di, pch=19, cex=0.2)
text(di$Taxes,di$NDIR,row.names(di), cex=0.4)
g = lm(NDIR~Taxes, data=di)
abline(g)
@
        \caption{Immigration vs. taxes}
        \end{figure}
		This problem concerns the data available at the location specified in the R code below:

        \verb|> di = read.table("http://pages.uoregon.edu/dlevin/DATA/DI.txt",|

        \verb|+                 header=T,row.names=1,sep="\t")|

        \verb|> plot(NDIR~Taxes, data=di, pch=19, cex=0.2)|

        \verb|> text(di$Taxes,di$NDIR,row.names(di), cex=0.4)|

        \verb|> g = lm(NDIR~Taxes, data=di)|

        \verb|> abline(g)|

		Figure 1 is a scatterplot of net immigration to states against income tax.
		(The data is aggregrated over a few years in the early 90's.)
		Do people move because of tax rates?
		Use the data in the file (see above) to discuss this question.
		\begin{proof}[Solution]
		Upon inspection of the regression of Taxes onto NDIR illustrated in 1, we observe that there appears to be negative correlation.
		Looking at the summary of this model (Table 1) we observe that none of the variables are highly significant, including taxes.
<<2.SummaryTab1, echo=FALSE, results="asis">>=
    xtable(mod,
           label = "Summary of Big Model",
           caption = "Summary of Big Model")
@
		Imagine you are an overly taxed New Yorker looking to move, are you more likely to traverse the Continental United States for a lessened tax burden, or over the border say Pennsylvania, or Vermont?
		Suppose we treat region as a categorical variable and fit different slopes and intercepts according to region.
		\begin{figure}[h]
<<2.Fig2, out.width='.7\\linewidth', echo = FALSE, fig = TRUE>>=
plot(NDIR~Taxes, data = di, col = Region, pch=19, cex=0.5)
f1 <- lm(NDIR~Taxes+Region+Taxes*Region, data = di)
betahat <- f1$coef
betahat.base <- f1$coef[1:2]
abline(betahat.base)
for(i in 0:3){
    abline(betahat.base+betahat[c(2+i,5+i)], col=1+i)
}
legend(3250,100,pch = 19, col = 1:4, legend = levels(di$Region))
@
        \caption{Immigration vs. taxes, by region}
        \end{figure}
        Figure 2 seems to indicate that one's tax burden will spur them to move, depending on where they live.
        Figure 3 illustrates that the regions which are experiencing the most out-migration are the two regions which have the highest tax burden in favor of regions with a lower tax burden.
        We can observe the ANOVA table in Table 2 to see that indeed, when split by the category of region, people may in fact move because of taxes.
<<2.ANOVA, echo=FALSE, results="asis">>=
mod1 <- lm(NDIR~1, data = di)
mod2 <- lm(NDIR~1+Taxes, data = di)
mod3 <- lm(NDIR~1+Taxes+Region, data = di)
mod4 <- lm(NDIR~1+Taxes+Region+Taxes*Region, data = di)
an <- anova(mod1,mod2,mod3,mod4)
xtable(an,
       label = "ANOVA",
       caption = "ANOVA")
@
        \begin{figure}[h]
<<2.Fig3, out.width='.7\\linewidth', echo = FALSE, fig = TRUE>>=
plot(Taxes~Region, data = di)
@
        \caption{Region as Taxes}
        \end{figure}
        Some concerns that I have with the data in question are that factors such as income and wage may be correlated, if not linearly then in some fashion.
        I have the same concern with unemployment, business failure rate, and education.
        It also seems likely that the errors here are not necessarily independent of the variables.
        Many things have an effect on factors such as business failure rate, and education that are not taken into consideration by the model.
        Moreover, factors which might serve to answer the question posed in this problem could be things like the rate of change of the GDP of a state, the robust-ness of the welfare state, and the type of taxes which are being summed into the taxes variable.
        It seems unlikely that renters would be effected by increases in property tax, whereas they would be effected by increases in income, or sales tax.
        This question may be more easily answered given a newer set of observational data, especially given that Kansas has recently undergone massive tax cuts, effectively running an experiment for us.
		\end{proof}
		% subsection problem_2 (end)
		\newpage

		\subsection{Problem 3} % (fold)
		\label{sub:problem_3}
		Recall that Instrumental Variables Least Squares (IVLS) requires variables $\bvec{Z}$ which are independent of the error terms $\bvec{\ep}$.
		(Such variables are called \emph{exogenous}.)
		Succesful application hinges on this assumption.
		Can this be verified from the data?
		This problem explores this question.
		Suppose that
		\begin{equation} \label{Eq:YXE}
		\bvec{Y} = \bvec{Z}\bvec{\alpha} + \bvec{X}\bvec{\beta} + \bvec{\ep} \,.
		\end{equation}
		Assume $\mathbb{E}[\bvec{\ep}] = 0$.
		Let $\bvec{Z}$ be a random $n$-vector, and suppose that $\cov(Z_i,\ep_i) = \rho$.
		The triples $(Z_i,X_i,\ep_i)$ are i.i.d. as triples for $i=1,2,\ldots,n$.
		\begin{enumerate}[(a)]
		\item Show that	\begin{equation} \label{Eq:Zep}	n^{-1} \sum_{i=1}^n Z_i \ep_i \to \rho \,. \end{equation}
		\item Show that if $n$ is large enough, \emph{and you can observe $\bvec{\ep}$}, you can test $H_0: \rho = 0$ with power $0.99$ against the alternative $H_1: |\rho| > 0.001$.
		\emph{Hint}: By the CLT, the test statistic
		\[
		\sqrt{n}\left( n^{-1} \sum_{i=1}^n Z_i \ep_i - \rho \right) \approx N(0,\kappa)
		\]
		where $\kappa = \var(Z_1\ep_1)$.
		Thus, with enough data, you can determine with high probability if the errors $\bvec{\ep}$ are correlated with $\bvec{Z}$.
		(Provided you can observe $\bvec{\ep}$.
		In most applications, $\bvec{\ep}$ is unobservable, however.)
		\item Let $\bvec{e}$ be the residuals from the OLS fit in \eqref{Eq:YXE}.
		Find the limit
		\[
		\lim_{n \to \infty} n^{-1} \sum_{i=1}^n Z_i e_i = \lim_{n \to \infty} n^{-1} \langle \bvec{Z}, \bvec{e} \rangle \,.
		\]
		Is it the same as the limit in \eqref{Eq:Zep}?
		\item Can you then use the residuals $\bvec{e}$ to determine $\cov(Z_1,\ep_1)$?
		\item If ``no'' what does this say about the ability to verify exogeniety (independence from error term) of instrumental variables?
		\end{enumerate}
		\begin{proof}[Solution] \
			\begin{enumerate}[(a)]
				\item
				\item
				\item
				\item
				\item
			\end{enumerate}
		\end{proof}
		% subsection problem_3 (end)
        \newpage

		\subsection{Problem 4} % (fold)
		\label{sub:problem_4}
		Suppose that
		\[
		\bvec{Y} = \bvec{X}\bvec{\beta} + \bvec{\ep}\,.
		\]
		The variables $\bvec{Z}_1, \bvec{Z}_2$ are instruments used to estimate $\bvec{\beta}$.
		Let $\tilde{\bvec{\beta}}$ denote the IVLS estimator of $\bvec{\beta}$.
		Let $\hat{\bvec{\beta}}$ denote the OLS estimator of $\bvec{\beta}$.
		\begin{enumerate}[(a)]
		\item If $n = 10, \bvec{\beta} = (0.2,0.5)$ and $\sigma = 1$, use simulation to estimate the mean-square error of both $\tilde{\bvec{\beta}}$ and $\hat{\bvec{\beta}}$:
		\[
		\sqrt{\mathbb{E}_{\bvec{\beta}}[\|\tilde{\bvec{\beta}} - \bvec{\beta}\|^2]},
		\quad
		\sqrt{\mathbb{E}_{\bvec{\beta}}[\|\hat{\bvec{\beta}} - \bvec{\beta}\|^2]}
		\]
		Do this for $\rho = 0.8, 0.3, 0$.
		\item Do the same for $n=10000$.  Repeat both for $\tau = 50$.
		\item For $n=10$ and $n=10000$: Estimate the standard errors for both estimators.  Which one
		is larger?  Estimate the bias for both estimators.  Which one is
		larger?
		\item Which estimator is better when $n=10$.  When $n= 100$? When $n=100000$?
		\end{enumerate}
		\begin{proof}[Solution]
<<4.Work, include=FALSE>>=
rm(list=ls())
library(expm)
set.seed(935867)

##### Simulation1 for n=10, rho=0.8, tau=1 #####
## OLS ##
n <- 10
sig <- 1
tau <- 1
rho <- .8

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS1 <- beta.hat - 0.5
# variance
var.OLS1 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS1 <- sqrt(var.OLS1)
# MSE
MSE.bh1 <- sqrt(var.OLS1 + bias.OLS1**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS1 <- beta.tilde - 0.5
# variance
var.IVLS1 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS1 <- sqrt(var.IVLS1)
# MSE
MSE.bt1 <- sqrt(var.IVLS1 + bias.IVLS1**2)

##### Simulation2 for n=10, rho=0.3, tau=1 #####
## OLS ##
n <- 10
sig <- 1
tau <- 1
rho <- .3

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS2 <- beta.hat - 0.5
# variance
var.OLS2 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS2 <- sqrt(var.OLS2)
# MSE
MSE.bh2 <- sqrt(var.OLS2 + bias.OLS2**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS2 <- beta.tilde - 0.5
# variance
var.IVLS2 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS2 <- sqrt(var.IVLS2)
# MSE
MSE.bt2 <- sqrt(var.IVLS2 + bias.IVLS2**2)

##### Simulation3 for n=10, rho=0, tau=1 #####
## OLS ##
n <- 10
sig <- 1
tau <- 1
rho <- 0

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS3 <- beta.hat - 0.5
# variance
var.OLS3 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS3 <- sqrt(var.OLS3)
# MSE
MSE.bh3 <- sqrt(var.OLS3 + bias.OLS3**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS3 <- beta.tilde - 0.5
# variance
var.IVLS3 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS3 <- sqrt(var.IVLS3)
# MSE
MSE.bt3 <- sqrt(var.IVLS3 + bias.IVLS3**2)

##### Simulation4 for n=10000, rho=0.8, tau=1 #####
## OLS ##
n <- 10000
sig <- 1
tau <- 1
rho <- .8

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS4 <- beta.hat - 0.5
# variance
var.OLS4 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS4 <- sqrt(var.OLS4)
# MSE
MSE.bh4 <- sqrt(var.OLS4 + bias.OLS4**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS4 <- beta.tilde - 0.5
# variance
var.IVLS4 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS4 <- sqrt(var.IVLS4)
# MSE
MSE.bt4 <- sqrt(var.IVLS4 + bias.IVLS4**2)

##### Simulation5 for n=10000, rho=0.3, tau=1 #####
## OLS ##
n <- 10000
sig <- 1
tau <- 1
rho <- .3

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS5 <- beta.hat - 0.5
# variance
var.OLS5 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS5 <- sqrt(var.OLS5)
# MSE
MSE.bh5 <- sqrt(var.OLS5 + bias.OLS5**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS5 <- beta.tilde - 0.5
# variance
var.IVLS5 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS5 <- sqrt(var.IVLS5)
# MSE
MSE.bt5 <- sqrt(var.IVLS5 + bias.IVLS5**2)

##### Simulation6 for n=10000, rho=0, tau=1 #####
## OLS ##
n <- 10000
sig <- 1
tau <- 1
rho <- 0

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS6 <- beta.hat - 0.5
# variance
var.OLS6 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS6 <- sqrt(var.OLS6)
# MSE
MSE.bh6 <- sqrt(var.OLS6 + bias.OLS6**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS6 <- beta.tilde - 0.5
# variance
var.IVLS6 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS6 <- sqrt(var.IVLS6)
# MSE
MSE.bt6 <- sqrt(var.IVLS6 + bias.IVLS6**2)

##### Simulation7 for n=10, rho=0.8, tau=50 #####
## OLS ##
n <- 10
sig <- 1
tau <- 50
rho <- .8

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS7 <- beta.hat - 0.5
# variance
var.OLS7 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS7 <- sqrt(var.OLS7)
# MSE
MSE.bh7 <- sqrt(var.OLS7 + bias.OLS7**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS7 <- beta.tilde - 0.5
# variance
var.IVLS7 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS7 <- sqrt(var.IVLS7)
# MSE
MSE.bt7 <- sqrt(var.IVLS7 + bias.IVLS7**2)

##### Simulation8 for n=10, rho=0.3, tau=50 #####
## OLS ##
n <- 10
sig <- 1
tau <- 50
rho <- .3

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS8 <- beta.hat - 0.5
# variance
var.OLS8 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS8 <- sqrt(var.OLS8)
# MSE
MSE.bh8 <- sqrt(var.OLS8 + bias.OLS8**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS8 <- beta.tilde - 0.5
# variance
var.IVLS8 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS8 <- sqrt(var.IVLS8)
# MSE
MSE.bt8 <- sqrt(var.IVLS8 + bias.IVLS8**2)

##### Simulation9 for n=10, rho=0, tau=50 #####
## OLS ##
n <- 10
sig <- 1
tau <- 50
rho <- 0

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS9 <- beta.hat - 0.5
# variance
var.OLS9 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS9 <- sqrt(var.OLS9)
# MSE
MSE.bh9 <- sqrt(var.OLS9 + bias.OLS9**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS9 <- beta.tilde - 0.5
# variance
var.IVLS9 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS9 <- sqrt(var.IVLS9)
# MSE
MSE.bt9 <- sqrt(var.IVLS9 + bias.IVLS9**2)

##### Simulation10 for n=10000, rho=0.8, tau=50 #####
## OLS ##
n <- 10000
sig <- 1
tau <- 50
rho <- .8

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS10 <- beta.hat - 0.5
# variance
var.OLS10 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS10 <- sqrt(var.OLS10)
# MSE
MSE.bh10 <- sqrt(var.OLS10 + bias.OLS10**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS10 <- beta.tilde - 0.5
# variance
var.IVLS10 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS10 <- sqrt(var.IVLS10)
# MSE
MSE.bt10 <- sqrt(var.IVLS10 + bias.IVLS10**2)

##### Simulation11 for n=10000, rho=0.3, tau=50 #####
## OLS ##
n <- 10000
sig <- 1
tau <- 50
rho <- .3

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS11 <- beta.hat - 0.5
# variance
var.OLS11 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS11 <- sqrt(var.OLS11)
# MSE
MSE.bh11 <- sqrt(var.OLS11 + bias.OLS11**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS11 <- beta.tilde - 0.5
# variance
var.IVLS11 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS11 <- sqrt(var.IVLS11)
# MSE
MSE.bt11 <- sqrt(var.IVLS11 + bias.IVLS11**2)

##### Simulation12 for n=10000, rho=0, tau=50 #####
## OLS ##
n <- 10000
sig <- 1
tau <- 50
rho <- 0

z1 <- rnorm(n)
z2 <- rnorm(n)
SM <- matrix(c(sig^2, rho*sig*tau, rho*sig*tau, tau^2), byrow=T, nrow=2)
SME <- eigen(SM)
SMSR <- SME$vectors %*% diag(sqrt(SME$values)) %*% t(SME$vectors)
W <- matrix(rnorm(n*2), nrow=2)
de <- t(SMSR %*% W)

d <- de[,1]
e <- rnorm(n)

X.OLS <- z1 + 2*z2 + d
Y.OLS <- 0.5*(X.OLS) + e
beta.hat <- solve(t(X.OLS) %*% X.OLS) %*% t(X.OLS) %*% Y.OLS

# bias
bias.OLS12 <- beta.hat - 0.5
# variance
var.OLS12 <- 1^2*(solve(t(X.OLS)%*%X.OLS))
SE.OLS12 <- sqrt(var.OLS12)
# MSE
MSE.bh12 <- sqrt(var.OLS12 + bias.OLS12**2)

## IVLS ##
Z.IVLS <- cbind(z1,z2)
Zt.IVLS <- sqrtm(solve(t(Z.IVLS) %*% Z.IVLS))
X.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% X.OLS
Y.IVLS <- Zt.IVLS %*% t(Z.IVLS) %*% Y.OLS
beta.tilde <- solve(t(X.IVLS) %*% X.IVLS) %*% t(X.IVLS) %*% Y.IVLS
# bias
bias.IVLS12 <- beta.tilde - 0.5
# variance
var.IVLS12 <- 1^2*(solve(t(X.IVLS)%*%X.IVLS))
SE.IVLS12 <- sqrt(var.IVLS12)
# MSE
MSE.bt12 <- sqrt(var.IVLS12 + bias.IVLS12**2)
@

		We perform the requested simulations and produce the following results,
		    \begin{figure}[h]
		        \begin{tabular}{lll|c|c|c|c|c|c}
		            $n$ & $\rho$ & $\tau$ & MSE$(\hat{\bbeta})$ & MSE$(\tilde{\bbeta})$ & $\SE(\hat{\bbeta})$ & $\SE(\tilde{\bbeta})$ & Bias $\hat{\bbeta}$ & Bias $\tilde{\bbeta}$ \\
		            \hline
		            10 & 0.8 & 1 & \Sexpr{MSE.bh1} & \Sexpr{MSE.bt1} & \Sexpr{SE.OLS1} & \Sexpr{SE.IVLS1} & \Sexpr{bias.OLS1} & \Sexpr{bias.IVLS1} \\
		            10 & 0.3 & 1 & \Sexpr{MSE.bh2} & \Sexpr{MSE.bt2} & \Sexpr{SE.OLS2} & \Sexpr{SE.IVLS2} & \Sexpr{bias.OLS2} & \Sexpr{bias.IVLS2} \\
		            10 & 0 & 1 & \Sexpr{MSE.bh3} & \Sexpr{MSE.bt3} & \Sexpr{SE.OLS3} & \Sexpr{SE.IVLS3} & \Sexpr{bias.OLS3} & \Sexpr{bias.IVLS3} \\
		            10000 & 0.8 & 1 & \Sexpr{MSE.bh4} & \Sexpr{MSE.bt4} & \Sexpr{SE.OLS4} & \Sexpr{SE.IVLS4} & \Sexpr{bias.OLS4} & \Sexpr{bias.IVLS4} \\
		            10000 & 0.3 & 1 & \Sexpr{MSE.bh5} & \Sexpr{MSE.bt5} & \Sexpr{SE.OLS5} & \Sexpr{SE.IVLS5} & \Sexpr{bias.OLS5} & \Sexpr{bias.IVLS5} \\
		            10000 & 0 & 1 & \Sexpr{MSE.bh6} & \Sexpr{MSE.bt6} & \Sexpr{SE.OLS6} & \Sexpr{SE.IVLS6} & \Sexpr{bias.OLS6} & \Sexpr{bias.IVLS6} \\
		            10 & 0.8 & 50 & \Sexpr{MSE.bh7} & \Sexpr{MSE.bt7} & \Sexpr{SE.OLS7} & \Sexpr{SE.IVLS7} & \Sexpr{bias.OLS7} & \Sexpr{bias.IVLS7} \\
		            10 & 0.3 & 50 & \Sexpr{MSE.bh8} & \Sexpr{MSE.bt8} & \Sexpr{SE.OLS8} & \Sexpr{SE.IVLS8} & \Sexpr{bias.OLS8} & \Sexpr{bias.IVLS8} \\
		            10 & 0 & 50 & \Sexpr{MSE.bh9} & \Sexpr{MSE.bt9} & \Sexpr{SE.OLS9} & \Sexpr{SE.IVLS9} & \Sexpr{bias.OLS9} & \Sexpr{bias.IVLS9} \\
		            10000 & 0.8 & 50 & \Sexpr{MSE.bh10} & \Sexpr{MSE.bt10} & \Sexpr{SE.OLS10} & \Sexpr{SE.IVLS10} & \Sexpr{bias.OLS10} & \Sexpr{bias.IVLS10} \\
		            10000 & 0.3 & 50 & \Sexpr{MSE.bh11} & \Sexpr{MSE.bt11} & \Sexpr{SE.OLS11} & \Sexpr{SE.IVLS11} & \Sexpr{bias.OLS11} & \Sexpr{bias.IVLS11} \\
		            10000 & 0 & 50 & \Sexpr{MSE.bh12} & \Sexpr{MSE.bt12} & \Sexpr{SE.OLS12} & \Sexpr{SE.IVLS12} & \Sexpr{bias.OLS12} & \Sexpr{bias.IVLS12}
		        \end{tabular}
		    \end{figure}

		\end{proof}
		% subsection problem_4 (end)
		\newpage

		\subsection{Problem 5} % (fold)
		\label{sub:problem_5}
		Suppose that
		\[
		\bvec{Y} = \beta_0 \bvec{1} + \beta_1 \bvec{x} + \bvec{\ep} \,,
		\]
		where $\{\ep_i\}$ are uncorrelated, and $\var(\ep_i | x_{i}) = \sigma^{2} \times x_{i}^{2}$.
		\begin{enumerate}[(a)]
		\item Is the OLS estimator for $\beta_1$ unbiased?
		\item Are the standard errors reported for the OLS estimator correct? (That is, good estimates of the actual standard deviation of the OLS estimator when applied to data generated from this model.)
		Give an expression for the standard deviation of the OLS estimators for this model, in terms of $\sigma$ and $\bvec{x}$.
		\item  Write down explicitly the GLS estimator of $\bvec{\beta}$ in terms of $\bvec{Y}$ and $\bvec{x}$.
		\item Suppose that instead, $\var(\ep_i) = \sigma^2 a_i$, where $a_i$ is a constant that takes on one of four variables depending on a categorical variable $w_i$.
		Describe the strategy of the feasible GLS estimator.
		\item Suppose that $\{X_i\}_{i=1}^n$ are i.i.d.\ $N(0,1)$.
		Suppose also that $w_i$ is each equally likely to take on any of its four values.
		Assume that the truth is $(a_1,a_2,a_3,a_4) = (1,2,4,8)$.
		For $n=25$ and $n=1000$, use simulation to estimate the true standard error of the OLS estimator and the true standard error of the feasible GLS estimator (implement the strategy above.)
		Estimate the bias in the reported standard error when using OLS from the true standard error of the OLS estimate (is it zero?).
		Assume that $Y_i = 3.2 + 2.4x_i + \ep_i$ and $\sigma = 10$.
		\end{enumerate}
		\begin{proof}[Solution] \
			\begin{enumerate}[(a)]
				\item No, in order for the OLS estimator for any $\beta$ to be unbiased we must assume that $\bvec{\ep}_{i}$ are I.I.D., we do not make that assumption in this case.
				\item No, we proceed with the definition of $\var(\hat{\beta}|x)$ and find that the standard errors are not $\SE(\hat{\beta}) = \sigma^{2}\mathbb{I}_{n \times n}$.
				We compute the following, note that $\var(Y|X) = \var(\ep|X)$.
					\begin{align*}
						\var(\hat{\beta}|x) &= \var\left[ \left( X'X \right)^{-1} X'Y | X \right], \\
						&= \left( X'X \right)^{-1}X' \var\left[Y|X\right]\left[\left( X'X \right)^{-1}X'\right]', \\
						&= \left( X'X \right)^{-1}X' \var\left[\ep|X\right]\left[\left( X'X \right)^{-1}X'\right]', \\
						&= \left( X'X \right)^{-1}X' \sigma^{2}X^{2} \left[\left( X'X \right)^{-1}X'\right]', \\
						&= \sigma^{2} \left\{ \left( X'X \right)^{-1}X'X \cdot X\left[\left( X'X \right)^{-1}X'\right]'\right\}.
					\end{align*}
				It is at this point where my matrix algebra skills fail me as I am crunched for time.
				However, I am certain that the terms inside the brackets do not simplify to the identity matrix, thus the standard error is not $\sigma^{2}\mathbb{I}_{n \times n}$.
				\item
				\item
				\item
			\end{enumerate}
		\end{proof}
		% subsection problem_5 (end)
		\newpage

		\subsection{Problem 6} % (fold)
		\label{sub:problem_6}
		Suppose that
		\[
		\bvec{X} =
		\begin{bmatrix}
		1 & 1 & 1 & 1 \\
		1 & 1 & 1 & -1 \\
		1 & 1 & -1 & 1 \\
		1 & 1 & -1 & -1 \\
		1 & -1 & 1 & 1 \\
		1 & -1 & 1 & -1 \\
		1 & -1 & -1 & 1 \\
		1 & -1 & -1 & -1 \\
		\end{bmatrix}
		\]
		Let $\bvec{Y} = \bvec{X}\bvec{\beta} + \bvec{\ep}$.
		Assume the errors are i.i.d.\ $N(0,\sigma^2)$.
		Find $\sigma$  so that the power of the $F$-test of
		\[
		H_0: \beta_3=\beta_4 = 0
		\]
		is $0.95$ against the alternative $\beta_3 = \beta_4 = 0.1$.
		Do the same with the matrix
		\[
		\bvec{X} =
		\begin{bmatrix}
		-1.70& -1.45& -0.55& -0.85\\
		-0.09& -0.01&  0.02&  1.32\\
		-1.03& -1.27& -1.47&  0.24\\
		-0.49&  0.39& -1.26& -0.57\\
		-0.42& -2.25& -0.93&  0.02\\
		0.45&  0.66&  0.15&  1.41\\
		0.33 &0.33&  0.92& -0.36\\
		0.31 &-0.78&  0.72&  0.34\\
		\end{bmatrix}
		\]
		If the answer differs, explain why.
		\begin{proof}[Solution]
<<6.Work, include=FALSE>>=
rm(list=ls())

V1 <- c(1,1,1,1,1,1,1,1)
V2 <- c(1,1,1,1,-1,-1,-1,-1)
V3 <- c(1,1,-1,-1,1,1,-1,-1)
V4 <- c(1,-1,1,-1,1,-1,1,-1)
X <- data.frame(cbind(V1,V2,V3,V4))

V3perp <- residuals(lm(V3~.-V4, data = X))
V4perp <- residuals(lm(V4~.-V3, data = X))
d1.num <- ((0.1^2)*sum(V3perp^2) + (0.1^2)*sum(V4perp^2) + (2*0.1*0.1)*sum(V3perp*V4perp))

W1 <- c(-1.70, -0.09, -1.03, -0.49, -0.42, 0.45, 0.33, 0.31)
W2 <- c(-1.45, -0.01, -1.27, 0.39, -2.25, 0.66, 0.33, -0.78)
W3 <- c(-0.55, 0.02, -1.47, -1.26, -0.93, 0.15, 0.92, 0.72)
W4 <- c(-0.85, 1.32, 0.24, -0.57, 0.02, 1.41, -0.36, 0.34)
Z <- data.frame(cbind(W1,W2,W3,W4))

W3perp <- residuals(lm(W3~.-W4, data = Z))
W4perp <- residuals(lm(W4~.-W3, data = Z))
d2.num <- ((0.1^2)*sum(W3perp^2) + (0.1^2)*sum(W4perp^2) + (2*0.1*0.1)*sum(W3perp*W4perp))

fstar <- qf(0.95, 2, 4)

powah <- function(crit,del,sig){1-pf(crit,2,4,I(del/sig**2))}

# trial and error
sig1 <- 0.0648601
powah(fstar, d1.num, sig1)

sig2 <- 0.03552299
powah(fstar,d2.num,sig2)
@
        Given that the constraint that the power is 0.95, we can compute $\sigma$ by first determining the numerator of the non-centrality parameter $\delta$ and the appropriate critical value $F^{\star}$, and then using a custom R function to determine an appropriate $\sigma$ by trial and error.
        In order to compute the numerator of $\delta$, and critical value we do the following in R
<<6.Perps, eval = FALSE, echo=TRUE>>=
V3perp <- residuals(lm(V3~.-V4, data = X))
V4perp <- residuals(lm(V4~.-V3, data = X))
d1.num <- ((0.1^2)*sum(V3perp^2) + (0.1^2)*sum(V4perp^2)
           + (2*0.1*0.1)*sum(V3perp*V4perp))

fstar <- qf(0.95, 2, 4)
@
        Doing this yields that the numerator of $\delta$ for the first matrix is $\delta_{\rm Num} = \Sexpr{d1.num}$ and for the second matrix, $\delta_{\rm Num} = \Sexpr{d2.num}$.
        We then use \verb|d1.num|, and $F^{\star} = $ \verb|fstar| in a custom function to trial and error our way to an appropriate $\sigma$.
<<6.Function, eval = FALSE, echo=TRUE>>=
powah <- function(crit,del,sig){1-pf(crit,2,4,I(del/sig**2))}
@
        After much trial and error we find that a reasonable standard deviation for the first model is $\sigma_{1} \doteq \Sexpr{sig1}$ and for the second model $\sigma_{2} \doteq \Sexpr{sig2}$.
        I suspect that these differ because the models in question appear to have little in common beyond their dimension.
		\end{proof}
		% subsection problem_6 (end)
	% section assignment (end)
\end{document}